custom:

  # Cluster configs for each environment
  default-cluster-spec: &default-cluster-spec
    spark_version: '11.0.x-cpu-ml-scala2.12'
#    node_type_id: 'i3.xlarge'
#    driver_node_type_id: 'i3.xlarge'
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
    driver_instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'
    instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *default-cluster-spec

# Databricks Jobs definitions
# please note that we're using FUSE reference for config, and env files, hence we're going to load this file using its local FS path
environments:

  dev:
    strict_path_adjustment_policy: true
    jobs:
      - name: 'DEV-telco-churn-demo-setup'
        <<: *dev-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/demo_setup_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
      - name: 'DEV-telco-churn-feature-table-creation'
        <<: *dev-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/feature_table_creator_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
      - name: 'DEV-telco-churn-model-train'
        <<:
          - *dev-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_train_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'DEV-telco-churn-model-deployment'
        <<:
          - *dev-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_deployment_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
      - name: 'DEV-telco-churn-model-inference-batch'
        <<:
          - *dev-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_inference_batch_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']
      - name: 'DEV-telco-churn-sample-integration-test'
        <<:
          - *dev-cluster-config
        spark_python_task:
          python_file: 'file://tests/integration/sample_test.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  staging:
    strict_path_adjustment_policy: true
    jobs:
      - name: 'STAGING-telco-churn-sample-integration-test'
        <<:
          - *staging-cluster-config
        spark_python_task:
          python_file: 'file://tests/integration/sample_test.py'
          parameters: ['--env', 'file:fuse://conf/staging/.staging.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  prod:
    strict_path_adjustment_policy: true
    jobs:
      - name: 'PROD-telco-churn-demo-setup'
        <<: *prod-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/demo_setup_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
      - name: 'PROD-telco-churn-initial-model-train-register'
        tasks:
          - task_key: 'demo-setup'
            <<:
              - *prod-cluster-config
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/demo_setup_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
          - task_key: 'feature-table-creation'
            <<: *prod-cluster-config
            depends_on:
              - task_key: 'demo-setup'
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/feature_table_creator_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
          - task_key: 'model-train'
            <<: *prod-cluster-config
            depends_on:
              - task_key: 'demo-setup'
              - task_key: 'feature-table-creation'
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/model_train_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'PROD-telco-churn-model-train'
        <<:
          - *prod-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_train_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'PROD-telco-churn-model-deployment'
        <<:
          - *prod-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_deployment_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
      - name: 'PROD-telco-churn-model-inference-batch'
        <<:
          - *prod-cluster-config
        spark_python_task:
          python_file: 'file://telco_churn/pipelines/model_inference_batch_job.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']